---
title: "Beginner's Guide to Text Mining & Sentiment Analysis using R"
author: "Kuldeep Singh Chouhan"
date: "`r Sys.Date()`"
output: 
  html_document:
    number_section: no
    toc: yes
    theme: cosmo
    highlight: tango
    code_folding: hide
---

![](data/StarWars-Minimal-Wallpapers-6.png)

# **Introduction**

It is estimated that over 70% of potentially useable business information is unstructured, often in the form of text data. Text mining provides a collection of techniques that allow us to derive actionable insights from these data. In this kernel we are going to perform text mining to perform analysis on the Star Wars scripts from The Original Trilogy Episodes (IV, V and VI). We will use different types of visualization techniques. We are going to use tm and tidytext packages for text mining and analysis.we will use wordcloud and ggplot2 for visualization.

So what exactly is **Text Mining**? 

Well, **Text Mining** is the process of distilling actionable insights from text. **Text Mining** workflow involves following steps:

  * Problem definition
  * Identify text to be collected
  * Text Organization
  * Feature Extraction (word tokens or sentiments)
  * Analysis
  * Reach an insight
  
It is time to load our data. We will use **read_table()** to load our text data.

# **Loading Data & Packages**

```{r message=FALSE}

#load packages
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
library(plotrix)
library(ggthemes)
library(RWeka)
library(gridExtra)
library(reshape2)
library(viridisLite)

#load datasets
ep4 <- read.table("data/SW_EpisodeIV.txt")
ep5 <- read.table("data/SW_EpisodeV.txt")
ep6 <- read.table("data/SW_EpisodeVI.txt")

```

# **Preprocessing** {.tabset .tabset-fade .tabset-pills}

After loading our data the next step is to get it's glimpse and to perform some preprocessing steps in order to use it for further analysis.

First we need to create a new object that only consists of text column from our dataframe. Next step is to convert this vector containing the text data into a corpus. A **Corpus** is a collection of documents. We will use **VectorSource()** to interpret each element in our vector of text as a document. The output of this function is going to be *Source Object*. Now that we've converted our vector to a *Source object*, we pass it to another tm function **VCorpus()** to create our volatile corpus. The VCorpus object is a nested list, or list of lists. At each index of the *VCorpus object*, there is a PlainTextDocument object, which is a list containing actual text data, and some corresponding metadata. Also, we can create *Source Object* from dataframes as well with the help of **DataframeSource()**.

We will use common preprocessing functions in order to clean our corpus. The **tm** and **qdap** packages have some common preprocessing functions to clean our corpus according to our requirements. Let's see some common functions from **tm** package:

* tolower() - make all text lowercase.
* removePunctuation() - removes punctuations like periods and exclamation points.
* removeNumbers() - removes numbers.
* removeWords() - remove specific words like ("the", "of", "is", etc.)
* stripWhiteSpace() - removes tabs and extra spaces.

Some cleaning functions from **qdap** package:

* bracketX(): Remove all text within brackets (e.g. "It's (so) cool" becomes "It's cool")
* replace_number(): Replace numbers with their word equivalents (e.g. "2" becomes "two")
* replace_abbreviation(): Replace abbreviations with their full text equivalents (e.g. "Sr" becomes "Senior")
* replace_contraction(): Convert contractions back to their base words (e.g. "shouldn't" becomes "should not")
* replace_symbol() Replace common symbols with their word equivalents (e.g. "$" becomes "dollar")

**NOTE: Specific preprocessing steps will vary based on the project.**

We will create some useful functions which will perform preprocessing on our vector corpus. The tm package provides a function tm_map() to apply cleaning functions to an entire corpus, making the cleaning steps easier. tm_map() takes two arguments, a corpus and a cleaning function. For compatibility, base R and qdap functions need to be wrapped in content_transformer().

## cleanCorpus

* cleanCorpus() - The first function performs cleaning and preprocessing steps to a corpus. A Corpus is a set of texts. It takes text vector as input and returns a cleaned corpus. 

```{r message=FALSE, warning=FALSE}

# Text transformations
cleanCorpus <- function(corpus){

  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, content_transformer(tolower))
  v_stopwords <- c(stopwords("english"), c("thats","weve","hes","theres","ive","im",
                                           "will","can","cant","dont","youve","us",
                                           "youre","youll","theyre","whats","didnt"))
  corpus <- tm_map(corpus, removeWords, v_stopwords)
  return(corpus)

}

```

## frequentTerms

After getting the cleaned corpus we need to change the structure for further analysis.The foundation of the **Text Mining** is the Term Document Matrix or The Document Term Matrix. The TDM has each corpus word represented as row and document as column. Both the matrices conatin the word frequencies. We will create Term Document Matrix using the function **TermDocumentMatrix()**.

* frequentTerms() - This function take text vector as input and return a dataframe with word and freq columns. This function constructs the term-document matrix from clean corpus, it describes the frequency of terms that occur in a collection of documents. This matrix has terms in the first column and documents (sentences) across the top as individual column names. 

```{r message=FALSE, warning=FALSE}

# Most frequent terms 
frequentTerms <- function(text){

  # VectorSource - Converting text vector into Source Object.
  # VCorpus - Converting source object into Volatile Corpus.
  # cleanCorpus - For cleaning our corpus.
  # TermDocumentMatrix - Turning Corpus into Term(row) Document(column) Matrix.
  # removeSparseTerms - To limit the number of words in our tdm.
  # as.matrix - To analyze TDM we need to change it to a simple matrix.
  # rowSums - Calling rowSums() on your newly made matrix aggregates all the terms used in a passage.
  # sort - sorting in decreasing order column wise.
  
  s.cor <- VCorpus(VectorSource(text))
  s.cor.cl <- cleanCorpus(s.cor)
  s.tdm <- TermDocumentMatrix(s.cor.cl)
  s.tdm <- removeSparseTerms(s.tdm, 0.999)
  m <- as.matrix(s.tdm)
  word_freqs <- sort(rowSums(m), decreasing=TRUE)
  dm <- data.frame(word=names(word_freqs), freq=word_freqs)
  return(dm)

}

```

## commonalityCloud

* commonalityCloud() - This function will take two different text vectors as arguments and it will make commonality cloud. The commonality cloud is a visualization which tells about the common words in two documents. 

```{r}

#commonality cloud function
commonalityCloud <- function(vec1, vec2){
  
  vec1 <- paste(vec1, collapse = "")
  vec2 <- paste(vec2, collapse = "")
  all_vec <- c(vec1, vec2)
  all_vec <- VCorpus(VectorSource(all_vec))
  all_clean <- cleanCorpus(all_vec)
  all_tdm <- TermDocumentMatrix(all_clean)
  all_m <- as.matrix(all_tdm)
  commonality.cloud(all_m, colors = "steelblue1", max.words = 35)
}

```

## comparisonCloud

* comparisonCloud() - Takes four arguments two different text vectors and the names of columns to be specified. A comparison cloud will tell us about the distinct words in two documents.

```{r}

# comparison cloud function
comparisonCloud <- function(vec1, vec2, col1, col2){
  
  vec1 <- paste(vec1, collapse = " ")
  vec2 <- paste(vec2, collapse = " ")
  all_vec <- c(vec1, vec2)
  all_vec <- VCorpus(VectorSource(all_vec))
  all_clean <- cleanCorpus(all_vec)
  all_tdm <- TermDocumentMatrix(all_clean)
  colnames(all_tdm) <- c(col1, col2)
  all_m <- as.matrix(all_tdm)
  comparison.cloud(all_m, colors = c("red", "blue"), max.words = 40, title.size = 1, scale = c(3, 1))
}

```


## pyramidPlot

* pyramidPlot - Commonality clouds show words that are shared across documents. One interesting thing that they can't show you is which of those words appear more commonly in one document compared to another. For this, we will use a **pyramid plot**, these can be generated using **pyramid.plot()** from the **plotrix** package.

```{r message=FALSE, warning=FALSE}

#function for constructing pyramidplot
pyramidPlot <- function(vec1, vec2, lab1, lab2){
  
  vec1 <- paste(vec1, collapse = " ")
  vec2 <- paste(vec2, collapse = " ")
  all_vec <- c(vec1, vec2)
  all_vec <- VCorpus(VectorSource(all_vec))
  all_clean <- cleanCorpus(all_vec)
  all_tdm <- TermDocumentMatrix(all_clean)
  all_m <- as.matrix(all_tdm)
  common <- subset(all_m, all_m[, 1] > 0 & all_m[, 2] > 0)
  difference <- abs(common[, 1] - common[, 2])
  common <- cbind(common, difference)
  common <- common[order(common[, 3], decreasing = TRUE), ]
  top20_df <- data.frame(x = common[1:20, 1],
                         y = common[1:20, 2],
                         labels = rownames(common[1:20, ]))
  pyramid.plot(top20_df$x, top20_df$y,
               labels = top20_df$labels,
               main = "Words in Common",
               gap = 8, raxlab = NULL,
               unit = NULL, 
               top.labels = c(lab1, "Words", lab2))
  
}

```


## frequentBigrams

We have TDM using single words only, but we can focus on tokens containing two words or more. This will help us to get more meaningful insights. 

* frequentBigrams() - It extracts token containing two words (bigrams). we will use this function two make plots for bigrams.

```{r message=FALSE, warning=FALSE}

# Tokenizer for defining bigrams
tokenizer  <- function(x){

  NGramTokenizer(x, Weka_control(min=2, max=2))

}

# Most frequent bigrams 
frequentBigrams <- function(text){

  s.cor <- VCorpus(VectorSource(text))
  s.cor.cl <- cleanCorpus(s.cor)
  s.tdm <- TermDocumentMatrix(s.cor.cl, control=list(tokenize=tokenizer))
  s.tdm <- removeSparseTerms(s.tdm, 0.999)
  m <- as.matrix(s.tdm)
  word_freqs <- sort(rowSums(m), decreasing=TRUE)
  dm <- data.frame(word=names(word_freqs), freq=word_freqs)
  return(dm)

}

```

# **EDA**

Now we will perform some exploratory data analysis on these datasets and then in the last we will combine these three datasets together to gain more meaningful insights. We will then perform some sentiment analysis. So let's dive in...

## Episode IV: A New Hope

We will have a glimpse of our dataset. 

```{r}
# getting glimpse of dataset.
glimpse(ep4)
```

There are two variables -

  * character - Number of character in Episode IV.
  
  * dialogue - dialogue given by particular character.
    
Let's get the total number of dialogue and characters in Episode IV.

```{r}
# Total number of characters.
ep4 %>%
  summarise(Total_Dialogue = n())

```

We can see there are total 60 character in Episode IV - A New Hope. We will now construct a frequency plot for showing top 15 characters.

```{r}
# Total number of dialogues.
length(ep4$dialogue)

```

There are 1010 dialogues in Episode IV. Now, We will get the total number of characters in Episode IV.

```{r}

top_char_ep4 <- ep4 %>%
  count(character) %>%
  mutate(character = fct_reorder(character,n)) %>%
  top_n(15,n)

ggplot(top_char_ep4, aes(character, n)) +
  geom_col(fill = "yellowgreen", color = "black") +
  coord_flip()+
  theme_gdocs()+
  labs(x="Characters", y="Number of dialogues",title="Star Wars Episode IV")+
  geom_label(label=top_char_ep4$n, size=3)

```

As we can see Luke is the top character in Episode IV followed by Han at second place and Threepio at third place. Hmm, seems nice.

Lets make a plot to see the most frequent terms in Episode IV- A New Hope. Instead of using wordcloud, I would like to use bar plot to analize the word frequency as they convey more clear and concise information.

```{r}

 frequentTerms(ep4$dialogue) %>%
  mutate(word = as.character(word)) %>%
  slice(1:15) %>%
  ggplot(aes(reorder(word,freq), freq)) +
  geom_col(fill = "burlywood", color = "black") +
  geom_label(aes(label = freq)) +
  coord_flip() +
  theme_gdocs() +
  labs(x="Word Frequency", y="Words",title="Star Wars Episode IV")

```

We can also analyze word frequency using **wordclouds**. A **wordcloud** is a visualization of terms. In a wordcloud, size is often scaled to frequency and in some cases the colors may indicate another measurement. We will use the **wordcloud** library to make wordclouds. 

```{r}

cloud <- frequentTerms(ep4$dialogue)
wordcloud(cloud$word, cloud$freq, max.words = 30, colors = cividis(5))

```

**Commonality cloud** will consists of words which are shared between two corporas. You can pass two text vectors in the function below to get the common words. 

```{r}

commonalityCloud(ep4$dialogue, ep5$dialogue)

```

We can also construct **Comaprison Cloud** to check the disjunction in two corpora. We can see below that the **Comaprison Cloud** identified the words that are dissimilar.

```{r}

comparisonCloud(ep4$dialogue, ep5$dialogue, "A New Hope", "The Empire Strikes Back")

```

We can also use **pyramid.plot()** from the **plotrix** library to plot common words in two corpora.

```{r message=FALSE, error=FALSE}

pyramidPlot(ep4$dialogue, ep5$dialogue, "A New Hope", "The Empire Strikes Back")

```


Now we will focus on tokens containing two words. This can help extract useful phrases which lead to some additional insights. Bigram tokenization grabs all two word combinations.

```{r}
frequentBigrams(ep4$dialogue) %>%
  mutate(word = as.character(word)) %>%
  slice(1:15) %>%
  ggplot(aes(reorder(word,freq), freq)) +
  geom_col(fill = "bisque4", color = "black") +
  geom_label(aes(label = freq)) +
  coord_flip() +
  theme_gdocs() +
  labs(x="Bigram Frequency", y="Bigrams",title="Star Wars Episode IV")
```

## Episode V-The Empire Strikes Back

Just like Episode IV, Epidoe V and Episode VI have two columsns.

  * character - Number of character in Episode V.
  
  * dialogue - dialogue given by particular character.

We will see the total number of characters and dialogues in Episode V. Let's Go...   

```{r}
# Total number of characters.
length(table(ep5$character))
```

```{r}
# Total number of dialogue
ep5 %>%
  summarise(Total_Dialogue = n())
```

So there are 49 character in Episode V and 839 dialogues. Slightly less than Episode IV. We will repeat the same procedure with Episode V for generating the plot to check the top characters and frequent terms & Bigrams.

```{r}
top_char_ep5 <- ep5 %>%
  count(character) %>%
  mutate(character = reorder(character,n)) %>%
  top_n(15,n)

ggplot(top_char_ep5, aes(character, n)) +
  geom_col(fill = "yellowgreen", color = "black") +
  coord_flip()+
  theme_gdocs()+
  labs(x="Character", y="Number of dialogues",  title="Star Wars Episode V")+
  geom_label(label=top_char_ep5$n, size=3)

```

```{r message=FALSE}
frequentTerms(ep5$dialogue) %>%
  mutate(word = as.character(word)) %>%
  slice(1:15) %>%
  ggplot(aes(reorder(word,freq), freq)) +
  geom_col(fill = "burlywood", color = "black") +
  geom_label(aes(label = freq)) +
  coord_flip() +
  theme_gdocs() +
  labs(x="Word Frequency", y="Words",title="Star Wars Episode V")

```

```{r}
frequentBigrams(ep5$dialogue) %>%
  mutate(word = as.character(word)) %>%
  slice(1:15) %>%
  ggplot(aes(reorder(word,freq), freq)) +
  geom_col(fill = "bisque4", color = "black") +
  geom_label(aes(label = freq)) +
  coord_flip() +
  theme_gdocs() +
  labs(x="Word Frequency", y="Bigrams",title="Star Wars Episode V")

```

## Episode VI-Return of the Jedi
  
We will see the total number of characters and dialogues in Episode VI. Let's Go...   

```{r}
# Total number of characters.
length(table(ep6$character))
```

Hmm, dialogues are much less in Episode 6 as compared to Episode 4 & Episode 5.

```{r}
# Total number of dialogues.
ep6 %>%
  summarise(Total_Dialogue = n())
```

Wow, there are 53 characters in Episode VI and 674 dialogues. We will repeat the same procedure with Episode VI for generating the plot to check the top characters and frequent terms & Bigrams.

```{r}
top_char_ep6 <- ep6 %>%
  count(character) %>%
  mutate(character = reorder(character,n)) %>%
  top_n(15,n)

ggplot(top_char_ep6, aes(character, n)) +
  geom_col(fill = "yellowgreen", color = "black") +
  coord_flip()+
  theme_gdocs()+
  labs(x="Character", y="Number of dialogues",           title="Star Wars Episode VI")+
  geom_label(label=top_char_ep6$n, size=3)

```

```{r message=FALSE}
frequentTerms(ep6$dialogue) %>%
  mutate(word = as.character(word)) %>%
  slice(1:15) %>%
  ggplot(aes(reorder(word,freq), freq)) +
  geom_col(fill = "burlywood", color = "black") +
  geom_label(aes(label = freq)) +
  coord_flip() +
  theme_gdocs() +
  labs(x="Word Frequency", y="Words",title="Star Wars Episode VI")

```


```{r}
frequentBigrams(ep6$dialogue) %>%
  mutate(word = as.character(word)) %>%
  slice(1:15) %>%
  ggplot(aes(reorder(word,freq), freq)) +
  geom_col(fill = "bisque4", color = "black") +
  geom_label(aes(label = freq)) +
  coord_flip() +
  theme_gdocs() +
  labs(x="Word Frequency", y="Bigrams",title="Star Wars Episode VI")

```

## Trilogy

Now we will combine all the three datasets to perform some more analysis. We will repeat the same steps as above to get more meaningful insights of the joint datasets. This is going to vbe awesome. 

```{r}
# The Original Trilogy 
trilogy <- rbind(ep4, ep5, ep6)
```

```{r}
# Number of character in trilogy
length(table(trilogy$character))
```

```{r}
# Total number of dialogues.
length(trilogy$dialogue)
```

Interesting! there are in total 2,523 dialogue in Star Wars Trilogy and 129 characters. We should now visualize the top character in whole trilogy.

```{r}

top_char_trilogy <- trilogy %>%
  count(character) %>%
  mutate(character = reorder(character,n)) %>%
  top_n(15,n)

ggplot(top_char_trilogy, aes(character, n)) +
  geom_col(fill = "yellowgreen", color = "black") +
  coord_flip()+
  theme_gdocs()+
  labs(x="Character", y="Number of dialogues", title="Star Wars Original Trilogy")+
  geom_label(label=top_char_trilogy$n, size=3)

```

Hmm, it seems like Luke is at the top of mountain. Hope to see something interesting in frequent term plot. :)

```{r}
frequentTerms(trilogy$dialogue) %>%
  mutate(word = as.character(word)) %>%
  slice(1:15) %>%
  ggplot(aes(reorder(word,freq), freq)) +
  geom_col(fill = "burlywood", color = "black") +
  geom_label(aes(label = freq)) +
  coord_flip() +
  theme_gdocs() +
  labs(x="Word Frequency", y="Words",title="Star Wars Original Trilogy")
```

Nice! Luke is the most frequent term in the trilogy. That's expected btw. :P

```{r}
frequentBigrams(trilogy$dialogue) %>%
  mutate(word = as.character(word)) %>%
  slice(1:15) %>%
  ggplot(aes(reorder(word,freq), freq)) +
  geom_col(fill = "bisque4", color = "black") +
  geom_label(aes(label = freq)) +
  coord_flip() +
  theme_gdocs() +
  labs(x="Word Frequency", y="Bigrams",title="Star Wars Original Trilogy")

```

# **Sentiment Analysis**

While word counts and visualizations suggest something about the content, we can do more. we move beyond word counts alone to analyze the sentiment or emotional valence of text.

Letâ€™s address the topic of opinion mining or sentiment analysis. When human readers approach a text, we use our understanding of the emotional intent of words to infer whether a section of text is positive or negative, or perhaps characterized by some other more nuanced emotion like surprise or disgust. We will use all the three datasets to perfrom sentiment analysis.

  * tokens - tokens consist of word-per-row.
  
  * sentiment - consists of bing sentiments.

```{r message=FALSE}
tokens <- trilogy %>%
  mutate(dialogue = as.character(trilogy$dialogue)) %>%
  unnest_tokens(word, dialogue) 

bing <- tokens %>%
  inner_join(get_sentiments("bing")) %>%
  count(sentiment, word) %>%
  filter(n >= 15) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word,n)) 

ggplot(bing, aes(word, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  ylab("Contribution to sentiment") +
  coord_flip() +
  theme_gdocs()
 
```

Let's also make a word cloud for Positive & Negative sentiments.

```{r message=FALSE}
tokens %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort=TRUE) %>%
  acast(word ~ sentiment, value.var="n", fill=0) %>%
  comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)

```

The nrc lexicon (from Saif Mohammad and Peter Turney) categorizes words in a binary fashion into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.

```{r message=FALSE}
pos.neg <- tokens %>%
  inner_join(get_sentiments("nrc")) %>%
  count(word, sentiment, sort = TRUE)

ggplot(pos.neg, aes(reorder(sentiment, n, sum), n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  labs(x="Sentiment", y="Frequency") +
  coord_flip() +
  theme_gdocs() 
  
```

We can use this lexicon to compute the most frequent words for each sentiment.

```{r}
pos.neg %>%
  group_by(sentiment) %>%
  arrange(desc(n)) %>%
  slice(1:10) %>%
  ggplot(aes(reorder(word,n), n)) +
  geom_col(aes(fill=sentiment), show.legend=FALSE) +
  facet_wrap(~sentiment, scales="free_y") +
  labs(y="Frequency", x="Terms") +
  coord_flip() +
  theme_gdocs() 

```

# **References**
*Courses*

Datacamp. Text Mining: Bag of Words. URL: https://www.datacamp.com/courses/intro-to-text-mining-bag-of-words

Datacamp. Sentiment Analysis in R: The Tidy Way. URL: https://www.datacamp.com/courses/sentiment-analysis-in-r-the-tidy-way

Text Mining with R. 
URL:https://www.tidytextmining.com/

*Packages*

Hadley Wickham (2017). tidyverse:  tidyverse  packages for data manipulation, exploration and visualization that share a common design philosophy. R package version 1.2.1. URL: https://CRAN.R-project.org/package=tidyverse

Ingo Feinerer and Kurt Hornik (2017). tm: Text Mining Package. R package version 0.7-3. URL: https://CRAN.R-project.org/package=tm

Ian Fellows (2014). wordcloud: Word Clouds. R package version 2.5. URL: https://CRAN.R-project.org/package=wordcloud

Dawei Lang and Guan-tin Chien (2018). wordcloud2: Create Word Cloud by 'htmlwidget'. R package version 0.2.1. URL: https://CRAN.R-project.org/package=wordcloud2

Hadley Wickham (2017). reshape2: Flexibly Reshape Data: A Reboot of the Reshape Package. R package version 1.4.3. URL: https://CRAN.R-project.org/package=reshape2

Doug Ashton and Shane Porter (2016). radarchart: Radar Chart from 'Chart.js'. R package version 0.3.1. URL: https://CRAN.R-project.org/package=radarchart

